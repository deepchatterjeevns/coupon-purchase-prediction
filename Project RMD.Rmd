---
title: "Project Data Exploration"
output: html_document
---
```{r Loading Data, echo=FALSE,results='hide',message=FALSE}
system("ls ../input")
system("echo \n\n")
system("head ../input/*")

#################################################################################
# This script translates Japanese text to English in the data files
# and keeps the English translation in separate columns
#################################################################################

# Create master translation table from Japanese to English
coupon_list_train = read.csv("coupon_list_train.csv", as.is=T) # Source file the English list is keyed by
trans = data.frame(
  jp=unique(c(coupon_list_train$GENRE_NAME, coupon_list_train$CAPSULE_TEXT,
              coupon_list_train$large_area_name, coupon_list_train$ken_name,
              coupon_list_train$small_area_name)),
  en=c("Food","Hair salon","Spa","Relaxation","Beauty","Nail and eye salon","Delivery service","Lesson","Gift card","Other coupon","Leisure","Hotel and Japanese hotel","Health and medical","Other","Hotel","Japanese hotel","Vacation rental","Lodge","Resort inn","Guest house","Japanse guest house","Public hotel","Beauty","Event","Web service","Class","Correspondence course","Kanto","Kansai","East Sea","Hokkaido","Kyushu-Okinawa","Northeast","Shikoku","Chugoku","Hokushinetsu","Saitama Prefecture","Chiba Prefecture","Tokyo","Kyoto","Aichi Prefecture","Kanagawa Prefecture","Fukuoka Prefecture","Tochigi Prefecture","Osaka prefecture","Miyagi Prefecture","Fukushima Prefecture","Oita Prefecture","Kochi Prefecture","Hiroshima Prefecture","Niigata Prefecture","Okayama Prefecture","Ehime Prefecture","Kagawa Prefecture","Tokushima Prefecture","Hyogo Prefecture","Gifu Prefecture","Miyazaki Prefecture","Nagasaki Prefecture","Ishikawa Prefecture","Yamagata Prefecture","Shizuoka Prefecture","Aomori Prefecture","Okinawa","Akita","Nagano Prefecture","Iwate Prefecture","Kumamoto Prefecture","Yamaguchi Prefecture","Saga Prefecture","Nara Prefecture","Mie","Gunma Prefecture","Wakayama Prefecture","Yamanashi Prefecture","Tottori Prefecture","Kagoshima prefecture","Fukui Prefecture","Shiga Prefecture","Toyama Prefecture","Shimane Prefecture","Ibaraki Prefecture","Saitama","Chiba","Shinjuku, Takadanobaba Nakano - Kichijoji","Kyoto","Ebisu, Meguro Shinagawa","Ginza Shinbashi, Tokyo, Ueno","Aichi","Kawasaki, Shonan-Hakone other","Fukuoka","Tochigi","Minami other","Shibuya, Aoyama, Jiyugaoka","Ikebukuro Kagurazaka-Akabane","Akasaka, Roppongi, Azabu","Yokohama","Miyagi","Fukushima","Much","Kochi","Tachikawa Machida, Hachioji other","Hiroshima","Niigata","Okayama","Ehime","Kagawa","Northern","Tokushima","Hyogo","Gifu","Miyazaki","Nagasaki","Ishikawa","Yamagata","Shizuoka","Aomori","Okinawa","Akita","Nagano","Iwate","Kumamoto","Yamaguchi","Saga","Nara","Triple","Gunma","Wakayama","Yamanashi","Tottori","Kagoshima","Fukui","Shiga","Toyama","Shimane","Ibaraki"),
  stringsAsFactors = F)

# Append data with translated columns...

# COUPON_LIST_TRAIN.CSV
coupon_list_train = read.csv("coupon_list_train.csv", as.is=T) # Read data file to translate
names(trans)=c("jp","en_capsule") # Rename column
coupon_list_train=merge(coupon_list_train,trans,by.x="CAPSULE_TEXT",by.y="jp",all.x=T) # Join translation onto original data
names(trans)=c("jp","en_genre"); coupon_list_train=merge(coupon_list_train,trans,by.x="GENRE_NAME",by.y="jp",all.x=T)
names(trans)=c("jp","en_small_area"); coupon_list_train=merge(coupon_list_train,trans,by.x="small_area_name",by.y="jp",all.x=T)
names(trans)=c("jp","en_ken"); coupon_list_train=merge(coupon_list_train,trans,by.x="ken_name",by.y="jp",all.x=T)
names(trans)=c("jp","en_large_area"); coupon_list_train=merge(coupon_list_train,trans,by.x="large_area_name",by.y="jp",all.x=T)
write.csv(coupon_list_train, "coupon_list_train_en.csv", row.names = F)

# COUPON_AREA_TRAIN.CSV
coupon_area_train = read.csv("coupon_area_train.csv", as.is=T) 
names(trans)=c("jp","en_small_area"); coupon_area_train=merge(coupon_area_train,trans,by.x="SMALL_AREA_NAME",by.y="jp",all.x=T)
names(trans)=c("jp","en_pref"); coupon_area_train=merge(coupon_area_train,trans,by.x="PREF_NAME",by.y="jp",all.x=T)
write.csv(coupon_area_train, "coupon_area_train_en.csv", row.names = F)

# USER_LIST
user_list = read.csv("user_list.csv", as.is=T) 
names(trans)=c("jp","en_pref"); user_list=merge(user_list,trans,by.x="PREF_NAME",by.y="jp",all.x=T)
write.csv(user_list, "user_list_en.csv", row.names = F)

# COUPON_DETAIL_TRAIN.CSV
coupon_detail_train = read.csv("coupon_detail_train.csv", as.is=T) 
names(trans)=c("jp","en_small_area"); coupon_detail_train=merge(coupon_detail_train,trans,by.x="SMALL_AREA_NAME",by.y="jp",all.x=T)
write.csv(user_list, "user_detail_en.csv", row.names = F)

# COUPON_Visit_TRAIN.CSV
coupon_visit_train = read.csv("coupon_visit_train.csv", as.is=T) 

# Prefecture Location
prefecture_locations = read.csv("prefecture_locations.csv", as.is=T) 
names(trans)=c("jp","en_PREF_NAME"); prefecture_locations=merge(prefecture_locations,trans,by.x="ï..PREF_NAME",by.y="jp",all.x=T)
names(trans)=c("jp","en_prefectual_office"); prefecture_locations=merge(prefecture_locations,trans,by.x="PREFECTUAL_OFFICE",by.y="jp",all.x=T)
write.csv(user_list, "prefecture_locations_en.csv", row.names = F)

# You get the idea... can use this to translate any of the other files, too
```


```{r Joining Data}
library(plyr)
coupon_detail_train<-rename(coupon_detail_train,c("en_small_area"="user_small_area"))
user_list<-rename(user_list,c("en_pref"="user_en_pref"))
coupon_visit_train<-rename(coupon_visit_train,c("VIEW_COUPON_ID_hash"="COUPON_ID_hash"))
masterdata<-join(coupon_visit_train, coupon_list_train, by='COUPON_ID_hash')
masterdata<-subset(masterdata, select = -c(large_area_name,ken_name,small_area_name,GENRE_NAME,CAPSULE_TEXT) )
masterdata<-join(masterdata, user_list, by='USER_ID_hash')
masterdata<-subset(masterdata, select = -c(PREF_NAME) )
masterdata<-join(masterdata, coupon_detail_train, by='PURCHASEID_hash')
masterdata<- masterdata[ -c(37,39:41) ]

### Add age group
masterdata$Age.Group<-cut(masterdata$AGE, 
                       breaks = c(-Inf, 20, 30, 40, 50, 60, Inf), 
                       labels = c("Under 20", "20 to 29", "30 to 39", "40 to 49", "50 to 59", "More than 60"), 
                       right = FALSE)
###Separate out purchase data
purchase.masterdata<-subset(masterdata, PURCHASE_FLG==1)
###Separate out people who never buy any coupon
purchase.USERID<-coupon_detail_train$USER_ID_hash
nonpurchase.masterdata<-subset(masterdata,  ! USER_ID_hash %in% purchase.USERID )
unique(nonpurchase.masterdata$USER_ID_hash)
```

There are only 49 users who have never bought any coupon before. Data for non-buyers is not significant.

```{r Table}
library(ggplot2)
table(purchase.masterdata$SEX_ID)
table(purchase.masterdata$Age.Group)
ggplot(purchase.masterdata,aes(x=SEX_ID,fill=Age.Group)) + geom_bar(position="fill") + coord_flip()
```

More female users than male users
Majority of people who made a purchase are between 40 to 59
More female in age group except in "More than 60"

```{r Plots}
library(ggplot2)
library(plotly)
ggplot(purchase.masterdata,aes(x=en_capsule)) + geom_bar() + coord_flip()
ggplot(purchase.masterdata,aes(x=en_capsule,fill=SEX_ID)) + geom_bar(position="fill") + coord_flip() + facet_wrap(~Age.Group)
```

Delivery service is the most popular, followed by food and Other
More female ordering delivery service across all age group except "More than 60"
Food is 50:50 between the genders except for "50 to 59" and "More than 60" whereby there are more males

```{r discount}
ggplotly(ggplot(purchase.masterdata,aes(x=PRICE_RATE,fill=SEX_ID)) + geom_histogram(bins=10) + facet_wrap(~Age.Group))
ggplotly(ggplot(purchase.masterdata,aes(x=DISPPERIOD,fill=SEX_ID)) + geom_histogram(bins=10) + facet_wrap(~Age.Group))
```

Most popular discount rate is between 40% to 60%. 
4 to 8 days is the most popular display period, while 0 to 4 days is the second most popular
The coupons with the longest valid discount period discount is the most popular
