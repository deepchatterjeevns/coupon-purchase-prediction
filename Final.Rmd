---
title: "Coupon Prediction"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Business Case
Companies like Groupon help companies to generate demand by offering coupons to the consumers, usually offering them at a discounted rate. Consumers/users information could be collected and analyzed to recommend coupons that the users are likely to use. This analysis on coupon prediction woukd provide a wide range of applications as:        
1)	There is a cost to recommending a coupon      
  a.	Cost of emailing the consumers       
  b.	Opportunity cost of using the space on the app for other advertisements       
2)	There is a benefit to recommending the correct coupon         
  a.	Earning commission when the user use the coupon       
  b.	Having a good reputation of getting users to buy the recommended coupons      
In this case, we seek to use the Kaggle dataset on Pompare Coupon recommendation dataset to:      
1)	Understand what are some variables that incentivize people to purchase the coupons.       
2)	Predict how likely someone will purchase a coupon given that they view.        
Using the prediction, we can understand how likely a user will buy a coupon if they view it. Together with another model to understand the probability each user views the coupons, we can recommend coupons to the users. This aims to reduce the cost of emailing unsuitable coupons to the customers and earn greater commission as users used our recommended coupons. 


```{r, echo=FALSE, message=FALSE}
library(dplyr)
library(lubridate)
library(readr)
```

# Translation
Changing of Japanese names to english names forked and adapted from [andreas] (https://www.kaggle.com/anguyen/coupon-purchase-prediction/translate-everything-to-english-using-r)
```{r}
dir <- './data/'
# Create master translation table from Japanese to English
coupon_list_train = read.csv(paste0(dir,"coupon_list_train.csv"), as.is=T) # Source file the English list is keyed by
trans = data.frame(
  jp=unique(c(coupon_list_train$GENRE_NAME, coupon_list_train$CAPSULE_TEXT,
              coupon_list_train$large_area_name, coupon_list_train$ken_name,
              coupon_list_train$small_area_name)),
  en=c("Food","Hair salon","Spa","Relaxation","Beauty","Nail and eye salon","Delivery service","Lesson","Gift card","Other coupon","Leisure","Hotel and Japanese hotel","Health and medical","Other","Hotel","Japanese hotel","Vacation rental","Lodge","Resort inn","Guest house","Japanse guest house","Public hotel","Beauty","Event","Web service","Class","Correspondence course","Kanto","Kansai","East Sea","Hokkaido","Kyushu-Okinawa","Northeast","Shikoku","Chugoku","Hokushinetsu","Saitama Prefecture","Chiba Prefecture","Tokyo","Kyoto","Aichi Prefecture","Kanagawa Prefecture","Fukuoka Prefecture","Tochigi Prefecture","Osaka prefecture","Miyagi Prefecture","Fukushima Prefecture","Oita Prefecture","Kochi Prefecture","Hiroshima Prefecture","Niigata Prefecture","Okayama Prefecture","Ehime Prefecture","Kagawa Prefecture","Tokushima Prefecture","Hyogo Prefecture","Gifu Prefecture","Miyazaki Prefecture","Nagasaki Prefecture","Ishikawa Prefecture","Yamagata Prefecture","Shizuoka Prefecture","Aomori Prefecture","Okinawa","Akita","Nagano Prefecture","Iwate Prefecture","Kumamoto Prefecture","Yamaguchi Prefecture","Saga Prefecture","Nara Prefecture","Mie","Gunma Prefecture","Wakayama Prefecture","Yamanashi Prefecture","Tottori Prefecture","Kagoshima prefecture","Fukui Prefecture","Shiga Prefecture","Toyama Prefecture","Shimane Prefecture","Ibaraki Prefecture","Saitama","Chiba","Shinjuku, Takadanobaba Nakano - Kichijoji","Kyoto","Ebisu, Meguro Shinagawa","Ginza Shinbashi, Tokyo, Ueno","Aichi","Kawasaki, Shonan-Hakone other","Fukuoka","Tochigi","Minami other","Shibuya, Aoyama, Jiyugaoka","Ikebukuro Kagurazaka-Akabane","Akasaka, Roppongi, Azabu","Yokohama","Miyagi","Fukushima","Much","Kochi","Tachikawa Machida, Hachioji other","Hiroshima","Niigata","Okayama","Ehime","Kagawa","Northern","Tokushima","Hyogo","Gifu","Miyazaki","Nagasaki","Ishikawa","Yamagata","Shizuoka","Aomori","Okinawa","Akita","Nagano","Iwate","Kumamoto","Yamaguchi","Saga","Nara","Triple","Gunma","Wakayama","Yamanashi","Tottori","Kagoshima","Fukui","Shiga","Toyama","Shimane","Ibaraki"),
  stringsAsFactors = F)

# Append data with translated columns...

# COUPON_LIST_TRAIN.CSV
coupon_list_train = read.csv(paste0(dir,"coupon_list_train.csv"), as.is=T) # Read data file to translate
names(trans)=c("jp","en_capsule") # Rename column
coupon_list_train=merge(coupon_list_train,trans,by.x="CAPSULE_TEXT",by.y="jp",all.x=T) # Join translation onto original data
names(trans)=c("jp","en_genre"); coupon_list_train=merge(coupon_list_train,trans,by.x="GENRE_NAME",by.y="jp",all.x=T)
names(trans)=c("jp","en_small_area"); coupon_list_train=merge(coupon_list_train,trans,by.x="small_area_name",by.y="jp",all.x=T)
names(trans)=c("jp","coupon_pref"); coupon_list_train=merge(coupon_list_train,trans,by.x="ken_name",by.y="jp",all.x=T)
names(trans)=c("jp","en_large_area"); coupon_list_train=merge(coupon_list_train,trans,by.x="large_area_name",by.y="jp",all.x=T)
write.csv(coupon_list_train, paste0(dir,"coupon_list_train_en.csv"), row.names = F)

# COUPON_AREA_TRAIN.CSV
coupon_area_train = read.csv(paste0(dir,"coupon_area_train.csv"), as.is=T) 
names(trans)=c("jp","en_small_area"); coupon_area_train=merge(coupon_area_train,trans,by.x="SMALL_AREA_NAME",by.y="jp",all.x=T)
names(trans)=c("jp","en_pref"); coupon_area_train=merge(coupon_area_train,trans,by.x="PREF_NAME",by.y="jp",all.x=T)
write.csv(coupon_area_train, paste0(dir,"coupon_area_train_en.csv"), row.names = F)

# USER_LIST
user_list = read.csv(paste0(dir,"user_list.csv"), as.is=T) 
names(trans)=c("jp","user_pref"); user_list=merge(user_list,trans,by.x="PREF_NAME",by.y="jp",all.x=T)
write.csv(user_list, paste0(dir,"user_list_en.csv"), row.names = F)

# Detail train 
detail_train = read.csv(paste0(dir,"coupon_detail_train.csv"), as.is=T) 
names(trans)=c("jp","en_small_area")
detail_train=merge(detail_train,trans,by.x="SMALL_AREA_NAME",by.y="jp",all.x=T)
write.csv(detail_train, paste0(dir,"coupon_detail_train_en.csv"), row.names = F)

# Prefecture Location
prefecture_locations = read.csv(paste0(dir,"prefecture_locations.csv"), as.is=T) 
names(trans)=c("jp","en_PREF_NAME")
prefecture_locations=merge(prefecture_locations,trans,by.x="PREF_NAME",by.y="jp",all.x=T)
names(trans)=c("jp","en_prefectual_office"); prefecture_locations=merge(prefecture_locations,trans,by.x="PREFECTUAL_OFFICE",by.y="jp",all.x=T)
write.csv(prefecture_locations, paste0(dir,"prefecture_locations_en.csv"), row.names = F)

```

# Loading Dataframe and Background on Different Datasets
```{r}
dir <- './data/'
visit <- read_csv(paste0(dir,"coupon_visit_train.csv"))

# reduce view log size to 1 month of data
date1 <- as.POSIXct("2012-05-24 00:00:00")
date2 <- as.POSIXct("2012-06-24 00:00:00")
int <- interval(date1, date2)
visit <- visit[visit$I_DATE %within% int,]
write_csv(visit, paste0(dir,"coupon_view_1month.csv"))

area_train <- read.csv(paste0(dir,"coupon_area_train_en.csv")) #translated to en
list_train <- read.csv(paste0(dir,"coupon_list_train_en.csv")) #translated to en
user_list <- read.csv(paste0(dir,"user_list_en.csv"), na.strings=c("", NA))
pref_locations <- read.csv(paste0(dir,"prefecture_locations_en.csv"))
detail_train <- read.csv(paste0(dir,"coupon_detail_train_en.csv"))
```

## Understanding Coupon area_train
each COUPON_ID_hash can have multiple rows. selecting one example of COUPON_ID_hash reveals that each coupon can be listed for multiple locations.
```{r}
sum(duplicated(area_train$COUPON_ID_hash))
head(area_train[duplicated(area_train$COUPON_ID_hash),])
area_train[area_train$COUPON_ID_hash == "7d1ce87a632bc4a57cfb4fc4a895cced",]

# one coupn can be listed in more than one prefecture
head(table(area_train$COUPON_ID_hash,area_train$PREF_NAME))
```

## Understanding Coupon list_train
This dataset contains characteristics of each coupon. There are no duplicates. Each row is one coupon.
There is location information (ken, small area name, large area name) for each coupon in this data set. But each coupon can have multiple locations as seen in area_train.
```{r}
head(list_train)
cat("There are", sum(duplicated(list_train$COUPON_ID_hash)), "duplicate coupons in list_train")
```

## Understanding Coupon Detail_train
This data set contains purchase log of users. A quick look at the data reveals that one user can purchase the same coupon of the same location at different times, each generating a row in this data set.
```{r}
head(detail_train)
head(detail_train[duplicated(detail_train$USER_ID_hash),])
```

## Understanding User_list
Prefecture (not small area) is provided.
```{r}
head(user_list)
str(user_list)
```

# Data cleaning
There are missing prefecture data in user_list. Why are there missing residential addresses? Have these users purchased something?
```{r}
cat(sum(is.na(user_list$user_pref)), "users with missing pref location\n")
# Checking if all purchased coupons have a user residential area name

cat("purchases without user residential area recorded:", sum(is.na(detail_train$SMALL_AREA_NAME)),"\n" )

cat(sum(user_list[is.na(user_list$user_pref), "USER_ID_hash" ] %in% unique(detail_train[!is.na(detail_train$SMALL_AREA_NAME), "USER_ID_hash"])), "users without pref name in user_list but has small area residential name in detail_train\n")
```

Can we use SMALL_AREA_NAME to get prefecture name for the users without a residential prefecture recorded? SMALL_AREA_NAME is the "User redidential area name" according to Kaggle Data page, but a quick check revealed that the same user can be listed in multiple locations on the purchase log, yet have no registered address in the user list. How could that be?

```{r}
head(detail_train[,c("USER_ID_hash", "SMALL_AREA_NAME")], n=10)
```

Browsing some coupon pages revealed that coupons like restaurant discount tickets or physical products have to be delivered to an address.

```"Discount ticket" will be delivered to the designated address at the time of purchase by "Kuroneko DM flight"```

That explains multiple small areas associated with each user. Coupons could be delivered either to the user's or friend's/family's address. With this in mind, we can fill up missing prefectures in user_list using the most common prefecture the coupons are delivered to.
```{r}
# Get (small area name - prefecture name) association from list_train
small.area.pref <- unique(list_train[,c("en_small_area", "coupon_pref")])

# Merge with detail train to get prefecture for small area
detail_train <- merge(detail_train, small.area.pref, by = "en_small_area", all.x = TRUE)

# Get most frequent prefecture for each user from purchase log
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

mode.pref <- detail_train %>% 
  group_by(USER_ID_hash) %>%
  summarize(mode.pref = getmode(coupon_pref))

user_list <- merge(user_list, mode.pref, by = "USER_ID_hash", all.x = TRUE)

user_list$user_pref <- as.character(user_list$user_pref)
user_list$mode.pref <- as.character(user_list$mode.pref)
user_list$user_pref <- ifelse(is.na(user_list$user_pref), user_list$mode.pref, user_list$user_pref)
user_list$mode.pref <- NULL
```

# Preparing data for modeling
Rows from view log contain multiple views of the same coupon-user pair, because each user can view a coupon multiple times before they eventually purchase during another session, each recorded as one row. The model summarizes into unique coupon-user pairs, in that users who eventually bought will be classified as purchased.
```{r}
visit.info<- visit %>%
  group_by(VIEW_COUPON_ID_hash,USER_ID_hash)%>%
  summarise(purchase = max(PURCHASE_FLG), view_count = n())
```

Merge visit.info, user characteristics, coupon characteristics. Drop irrelevant columns to reduce dataframe size
```{r}
dat <-merge(list_train,visit.info, by.x = "COUPON_ID_hash",by.y = "VIEW_COUPON_ID_hash")
dat <-merge(dat, user_list, by="USER_ID_hash")

drops <- c("large_area_name","ken_name","small_area_name","GENRE_NAME", "DISPFROM","DISPEND","VALIDFROM","VALIDEND","PREF_NAME","REG_DATE" ,"WITHDRAW_DATE", "CAPSULE_TEXT", "en_large_area", "en_small_area")
dat <- dat[, !(names(dat) %in% drops)]
```

```{r}
write.csv(dat, paste0(dir,"visit_user_coupon.csv"), row.names = F)
```

# Feature Engineering
```{r}
# If starting from this section
# dat <- read.csv(paste0(dir,"visit_user_coupon.csv"))
```

## Distance between user and coupon default prefecture
```{r}
prefs <- pref_locations[,c("LATITUDE", "LONGITUDE", "en_PREF_NAME")]

names(prefs) <- c("coupon_LATITUDE", "coupon_LONGITUDE", "en_PREF_NAME")
dat <- merge(dat, prefs, by.x = "coupon_pref", by.y = "en_PREF_NAME", all.x = TRUE)

names(prefs) <- c("user_LATITUDE", "user_LONGITUDE", "en_PREF_NAME")
dat <- merge(dat, prefs, by.x = "user_pref", by.y = "en_PREF_NAME", all.x = TRUE)

dat$user.coupon.dist <- sqrt(I(dat$coupon_LATITUDE - dat$user_LATITUDE)^2 + I(dat$coupon_LONGITUDE - dat$user_LONGITUDE)^2)

drops_latlon <- c("coupon_LATITUDE","user_LATITUDE","coupon_LONGITUDE","user_LONGITUDE", "coupon_pref")
dat <- dat[, !(names(dat) %in% drops_latlon)]

```

## Most frequently visited genre
```{r}
mode.genre <- dat %>% 
  group_by(USER_ID_hash) %>%
  summarize(mode.visit.genre = getmode(en_genre))

dat <- merge(dat, mode.genre,by = "USER_ID_hash",all.x = TRUE)
```


## Most frequent gender viewing coupons
```{r}
mode.gender <- dat %>% 
  group_by(COUPON_ID_hash) %>%
  summarize(mode.visit.gender = getmode(SEX_ID))

dat <- merge(dat, mode.gender,by = "COUPON_ID_hash",all.x = TRUE)
```


## Most frequent age group viewing coupons
```{r}
dat$age_group<-cut(dat$AGE, 
                          breaks = c(15, 22, 30, 40, 50, 60, 85), 
                          labels = c("15 to 21", "22 to 29", "30 to 39", "40 to 49", "50 to 59", "60 to 85"), 
                          right = FALSE)
mode.agegroup <- dat %>% 
  group_by(COUPON_ID_hash) %>%
  summarize(mode.visit.agegroup = getmode(age_group))
dat <- merge(dat, mode.agegroup, by = "COUPON_ID_hash",all.x = TRUE)
```

## Weekend and Weekday flag 
Get weekend, weekday flags. NA is converted to 1 because if not given, it is usable every day.
```{r}
dat$USABLE_DATE_Weekday<-ifelse(dat$USABLE_DATE_FRI!=0|dat$USABLE_DATE_THU!=0|dat$USABLE_DATE_WED!=0|dat$USABLE_DATE_TUE!=0|dat$USABLE_DATE_MON!=0,1,0)
dat$USABLE_DATE_Weekday[is.na(dat$USABLE_DATE_Weekday)] <- 1
dat$USABLE_DATE_Weekend<-ifelse(dat$USABLE_DATE_SAT!=0|dat$USABLE_DATE_SUN!=0,1,0)
dat$USABLE_DATE_Weekend[is.na(dat$USABLE_DATE_Weekend)] <- 1
dat$USABLE_DATE_HOLIDAY<-ifelse(dat$USABLE_DATE_HOLIDAY!=0,1,0)
dat$USABLE_DATE_HOLIDAY[is.na(dat$USABLE_DATE_HOLIDAY)] <- 1
dat$USABLE_DATE_BEFORE_HOLIDAY<-ifelse(dat$USABLE_DATE_BEFORE_HOLIDAY!=0,1,0)
dat$USABLE_DATE_BEFORE_HOLIDAY[is.na(dat$USABLE_DATE_BEFORE_HOLIDAY)] <- 1

drops_dayofweek <- c("USABLE_DATE_MON","USABLE_DATE_TUE","USABLE_DATE_WED","USABLE_DATE_THU", "USABLE_DATE_FRI","USABLE_DATE_SAT","USABLE_DATE_SUN")
dat <- dat[, !(names(dat) %in% drops_dayofweek)]

dat$USABLE_DATE_HOLIDAY<-as.factor(dat$USABLE_DATE_HOLIDAY)
dat$USABLE_DATE_BEFORE_HOLIDAY<-as.factor(dat$USABLE_DATE_BEFORE_HOLIDAY)
dat$USABLE_DATE_Weekday<-as.factor(dat$USABLE_DATE_Weekday)
dat$USABLE_DATE_Weekend<-as.factor(dat$USABLE_DATE_Weekend)
```

```{r}
dat$discgroup <- ifelse(dat$PRICE_RATE <= 55, "LOW", "HIGH")
dat$discgroup <- as.factor(dat$discgroup)
dat$PRICE_RATE <- NULL
```

## If user prefecture is in one of the prefectures where coupon can be used in
```{r}
area_train$en_pref <- as.character(area_train$en_pref) 

area.train.list <- area_train %>%
  group_by(COUPON_ID_hash) %>%
  summarise(areas = list(en_pref))

dat<- merge(dat, area.train.list, by = "COUPON_ID_hash", all.x = T)
dat$match_pref <- ifelse(dat$user_pref %in% dat$areas, 1, 0)
dat$match_pref<-as.factor(dat$match_pref)
```

# Data Exploration
This section aims to explore the coupon data, based on various characteristics of the coupons. Exploration will be conducted for list prices of coupons first, followed by exploration for purchases of coupons.

## Coupon Exploration
### Most visited coupons 
```{r}
library(ggplot2)
ggplot(dat) + geom_bar(aes(x=mode.visit.genre), fill="orange") + coord_flip() + theme(axis.text.y=element_text(size = rel(0.8)))
```

Top 3 most visited coupon genres: Delivery service, Food, Hotel and Japanese hotel. 

### List price
(a) Relationship between discount rates and coupon genres, view/purchase activity
```{r}
ggplot(dat,aes(x=discgroup)) + geom_bar() + facet_wrap(~purchase) + coord_flip()
```

Having split the set of coupons into two almost equal groups of "LOW" and "HIGH" based on their discount rates, we then compared this to the level of user viewing and purchasing activity.  

Very intuitively, "HIGH" discount rate coupons were purchased more often, and this is despite the lower frequency of views. The level of discount offered is hence a compelling factor in the conversion of a "view" to a "purchase".

```{r}
ggplot(dat, aes(x=en_genre)) + geom_bar(aes(fill=discgroup), position="fill") + geom_point(aes(y=-0.05), size=0.75,alpha=0.3,position=position_jitter(h=0.01)) + ggtitle("Proportion of discount rates in each genre") + labs(x= "Genre", y="%") + theme(axis.text.x=element_text(angle=90,size=10,vjust = 0.5))
```

Splitting these viewing/purchase activity by genre, it is noted that lower discount rates are more commonly offered for these coupon genres: Food, Hotel and Japanese hotel, and Leisure. Referring to an earlier observation: Food, Hotel and Japanese hotel coupon genres are among the top visited coupons genre. This could  explain why lower discount rates are given for these genres since demand for these coupons are high based on visit frequencies. 
"HIGH" discount rates are more commonly offered for the other genres. Beauty coupon genre offers 100% "HIGH" discount rates. However, it can also be observed that Beauty coupons had 0 visits from the most visited coupon genres section.


(b) Relationship between list price and periods
Display Period
```{r}
ggplot(data=dat, aes(x=DISPPERIOD,y=CATALOG_PRICE,color=en_genre)) + geom_point() + xlim(0,15) + labs(x="Display Period", y="List Price") + ggtitle("List price against Display period")
```

Display period of coupons do not seem to have an effect on list price. A small effect is observed only when display period is for 1 day, then list price is substantially lower than when display period is for 2 days, as shown by the clustering of list prices in the scatterplot to the lower values when display period==1. 

```{r}
ggplot(data=dat, aes(x=DISPPERIOD,y=view_count,color=en_genre)) + geom_point() + xlim(0,15) + ylim(0,100) + labs(x="Display Period", y="No of views") + ggtitle("No of views against Display period")
```

It is not surprising to observe that the longer the display period, the more views a coupon would have. However, a surprising observation made is that the number of views decreased sharply as display period extends beyond 8 days.


### Valid Period
```{r}
ggplot(data=dat, aes(x=VALIDPERIOD,y=CATALOG_PRICE,color=en_genre)) + geom_point() + xlim(0,15) + ylim(0, 12500) + labs(x="Valid Period", y="List Price") + ggtitle("List price against Valid period")
```

No correlation is observed


### Purchased Coupons
(a) Relationship between Views and Purchases in each genre
```{r}
dat$purchase <- as.factor(dat$purchase)
ggplot(dat, aes(x=en_genre)) + geom_bar(aes(fill=purchase), position="fill") + geom_point(aes(y=-0.05), size=0.75,alpha=0.3,position=position_jitter(h=0.01)) + ggtitle("Proportion of purchase and non purchase in each genre") + labs(x= "Genre", y="%") + theme(axis.text.x=element_text(angle=90,size=10,vjust = 0.5))
```

Proportion of purchased coupons is low as compared to visits to a coupon without a purchase. 

```{r}
ggplot(dat[dat$purchase==1,], aes(x=en_genre)) + geom_bar(aes(fill=purchase)) + geom_point(aes(y=-0.05), size=0.75,alpha=0.3,position=position_jitter(h=0.01)) + ggtitle("Number of purchases in each genre") + labs(x= "Genre", y="Number") + theme(axis.text.x=element_text(angle=90,size=10,vjust = 0.5))
```

Top 3 purchased coupons: Delivery service, food, other coupon
This is reasonable given that delivery service and food coupons are amongst the top 3 visited coupon genres as well.

Looking solely at purchasing behaviour, viewing coupons in the hotels genres do not translate to as many purchases, as "Other" coupons have better sales following Delivery Service and Food, although they are not viewed relatively as much. This suggests that users possibly take a longer time to make a purchase in the hotels genres, viewing multiple coupons before deciding. 

(b) Relationship between purchase and periods
```{r}
ggplot(dat[dat$purchase==1,], aes(x=DISPPERIOD)) + geom_bar(aes()) + geom_point(aes(y=-0.05), size=0.75,alpha=0.3,position=position_jitter(h=0.01)) + ggtitle("Purchases by Display Periods") + labs(x= "Display period", y="Number") + theme(axis.text.x=element_text(angle=90,size=10,vjust = 0.5))
```

The highest number of purchases is observed when display period is at 4 days. It can be inferred that the optimal period to display a coupon for is 4 days.

Purchase flag v Valid period
```{r}
ggplot(dat[dat$purchase==1,], aes(x=VALIDPERIOD)) + geom_bar(aes()) + geom_point(aes(y=-0.05), size=0.75,alpha=0.3,position=position_jitter(h=0.01)) + ylim(0,500) + ggtitle("Purchases by Valid Periods") + labs(x= "Valid Period", y="Number") + theme(axis.text.x=element_text(angle=90,size=10,vjust = 0.5))
```

No observable pattern.

## User exploration
This section aims to explore user data, and how different user demographics infleunce their purchase and viewing behaviour on the website. 

### Age 
```{r}
ggplot(dat)+geom_density(aes(x=AGE)) + ggtitle("Density plot of the age of users")
```

```{r}
ggplot(dat) + geom_bar(aes(x=en_capsule,fill=age_group), position="fill") + facet_wrap(~purchase) + coord_flip() + labs(x="Genre", y="Count of purchases")
```

```{r}
ggplot(dat) + geom_boxplot(aes(x=en_genre,y=AGE)) + ggtitle("Boxplot of users' age by genre") + labs(x="Genre", y="Age") + theme(axis.text.x=element_text(angle=90,size=10,vjust = 0.5))
```

Overall, customers are mostly clustered between the 30-50 age group as observed in the bar chart. However, those in the 40-60 age range are more active in terms of level of behaviour on the Recruit Ponpare site, as shown in the density plot above.  

Charting various boxplots for each specific genre shows that Hotel and Japanese hotel, and Leisure coupon genrse have relatively higher age groups, albeit slightly, which is reasonable given that these coupon genres have lower discount rates. Users purchasing these coupon genres would need relatively higher spending power. Food coupons do not seem to have relatively higher age groups, which could be because food coupons are more generic and applicable to all age groups. 

### Gender
```{r}
ggplot(dat,aes(x=en_capsule,fill=SEX_ID)) + geom_bar(position="fill") + facet_wrap(~purchase) + coord_flip()
```

Unsurprisingly, one's gender does influence the genre of coupons viewed. As seen in the graph, female users view Hair Salon, Nail & Eye Salon, Beauty, Spa and Relaxation coupon genres more often than their male counterparts whose views and purchases in these genres are basically non-existent. 

### Are users buying from same prefectures?
```{r}
ggplot(dat[!is.na(dat$user_pref) & dat$purchase ==1,],aes(x=user_pref)) + geom_bar()  + coord_flip()
ggplot(area_train[!is.na(area_train$en_pref),], aes(x=en_pref)) + geom_bar() + coord_flip()
dat$purchase_oop <- ifelse(dat$purchase == 1 & dat$match_pref == 0,"Outside of Prefecture","Within Prefecture")
ggplot(dat[dat$purchase == 1,], aes(x=purchase_oop)) + geom_bar() + coord_flip()
dat$areas<- NULL
dat$purchase_oop<- NULL
```

# Creating the dataframe

## Creating columns for coupon-user characteristic match
```{r}
##Genre Match
dat$en_genre<-as.character(dat$en_genre)
dat$mode.visit.genre<-as.character(dat$mode.visit.genre)
dat$same.genre <- dat$en_genre==dat$mode.visit.genre

dat$en_capsule <- NULL
dat$mode.visit.genre <- NULL
dat$en_genre<-factor(dat$en_genre)

## If gender Match
dat$same.gender <- dat$SEX_ID == dat$mode.visit.gender
dat$mode.visit.gender<-NULL

## If age group Match
dat$same.age.group <- dat$age_group == dat$mode.visit.agegroup
dat$mode.visit.agegroup<-NULL
```

## Remove NAs
```{r}
dat<-dat %>% 
  group_by(en_genre) %>% 
  mutate(VALIDPERIOD = replace(VALIDPERIOD, is.na(VALIDPERIOD), mean(VALIDPERIOD, na.rm=TRUE)))

dat$user.coupon.dist <- as.numeric(dat$user.coupon.dist)
dat<-dat %>% 
  group_by(age_group) %>% 
  mutate(user.coupon.dist= replace(user.coupon.dist, is.na(user.coupon.dist), mean(user.coupon.dist, na.rm=TRUE)))

dat$user_pref <- as.character(dat$user_pref)
dat$user_pref[is.na(dat$user_pref)] <- "Unknown"
dat$user_pref <- as.factor(dat$user_pref)
```

Drop user and coupon IDs
```{r}
dat <-subset(dat,select = -c(USER_ID_hash, COUPON_ID_hash))
```

```{r}
write.csv(dat, paste0(dir,"visit_user_coupon_features.csv"), row.names = F)
```

# Modelling 
Our approach to modelling is:        
1)	Spilt data into 70% train, 30% test       
2)	Apply 10-fold cross validation on the training data       
3)	Apply various models on the cross validated training data      
4)	Evaluate different models using the performance measure       
a.	Performance measure chosen: Area under Precision Recall curve        
b.	Reason: Precision recall aims to measure how efficient and precise the model, and measure whether the coupons we recommended are suitable coupons to the users, and the number of coupons recommended.
5)	Select the best model and apply it to the dataset        
6)	Report the generalized performance of the model when new data is added (30% test)         



If reading new file:
```{r}
#dat <- read.csv(paste0(dir,"visit_user_coupon_features.csv"))
#dat$USABLE_DATE_HOLIDAY<-as.factor(dat$USABLE_DATE_HOLIDAY)
#dat$USABLE_DATE_BEFORE_HOLIDAY<-as.factor(dat$USABLE_DATE_BEFORE_HOLIDAY)
#dat$USABLE_DATE_Weekday<-as.factor(dat$USABLE_DATE_Weekday)
#dat$USABLE_DATE_Weekend<-as.factor(dat$USABLE_DATE_Weekend)
```

## Split into test and train dataset
```{r}
dat$purchase<-as.factor(dat$purchase)
```


```{r}
set.seed(123)
sample <- sample(sample(c(TRUE,FALSE),nrow(dat),prob = c(0.70,0.30),replace = TRUE))
test <- dat[!sample,]
train <- dat[sample,]

write.csv(train, paste0(dir,"visit_train.csv"), row.names = F)
write.csv(test, paste0(dir,"visit_test.csv"), row.names = F)
```


```{r}
library(caret)
library(caTools)
auc_pr<- function(obs, pred) {
  xx.df <- prediction(pred, obs)
  perf  <- performance(xx.df, "prec", "rec")
  xy    <- data.frame(recall=perf@x.values[[1]], precision=perf@y.values[[1]])
  
  # take out division by 0 for lowest threshold
  xy <- subset(xy, !is.nan(xy$precision))
  
  # Designate recall = 0 as precision = x...arbitrary
  xy <- rbind(c(0, 0), xy)
  #xy <- xy[!(rowSums(xy)==0), ]
  
  res   <- trapz(xy$recall, xy$precision)
  res
}
```


## Decision Tree
We used a decision tree because it provides a easy method to understand purchasing decision.
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
set.seed(4217)
dtree <- train(purchase ~ ., data = train, method = "rpart", trControl = ctrl, parms=list(split='information'))

library(rattle)
fancyRpartPlot(dtree$finalModel)

rpartProbs <- predict(dtree, train, type = "prob")
summary(rpartProbs)


library(ROCR)
pr <- prediction(rpartProbs$`1`, train$purchase)
tree<-performance(pr,measure= "prec", x.measure= "rec")
performance(pr,measure= "prbe")
plot(tree, col= "blue")
```


## Logistic Regression
Lasso Regression is used based on the objective to minimize variance and perform regularisation. 
In caret package, glmnet performed feature selections for logistic regression
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10,savePredictions = TRUE)
set.seed(4217)
log_fit <- train(purchase ~ .,  data=train, method="glmnet",trControl = ctrl)
predictors(log_fit)
var.imp<-varImp(log_fit)
plot(var.imp,top = 20)
log_pred <- predict(log_fit,newdata = train,type="prob")

pr <- prediction(log_pred$`1`, train$purchase)
log<-performance(pr,measure= "prec", x.measure= "rec")
plot(log, col= "orange")
```


## Naive Bayes
```{r, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv", number = 10,savePredictions = TRUE)
set.seed(4217)
nb.fit <- train(purchase ~ .,  data=train, method="nb",trControl = ctrl)
predictors(nb.fit)

nb.prob <- predict(nb.fit,newdata = train,type="prob")
nb.pr <- prediction(nb.prob$`1`, train$purchase)
nb<-performance(nb.pr,measure= "prec", x.measure= "rec")
performance(nb.pr,measure= "prbe")
plot(nb, col="red")
```


## KNN
KNN was not used in the end due to high number of observations which require the calculation of large numbers of distance for prediction. However, this could be one of a feasible models for prediction as the observations that are similar have similar purchasing decisions.  
```{r}
#ctrl <- trainControl(method = "repeatedcv", number = 10,savePredictions = TRUE)
#set.seed(4217)
#knn.fit <- train(purchase ~ .,  data=train, method="knn",trControl = ctrl)
#knn.prob <- predict(knn.fit,newdata = train,type="prob")
#knn.pr <- prediction(knn.prob$`1`, train$purchase)
#knn<-performance(knn.pr,measure= "prec", x.measure= "rec")
#plot(knn,  col="blue")   #add = TRUE, 
```


# Model evaluation 
```{r}
plot(log,col="blue")
plot(tree, col="green", add= TRUE)
plot(nb, col= "red", add = TRUE)
#plot(knn, col = blue", add = TRUE)
legend('bottomleft', c("rpart_tree","NB","log") , 
       lty=1, col=c('green','red',"blue"), bty='n', cex=.75)
cat("auc for tree:", auc_pr(train$purchase, rpartProbs$`1`),"\n","auc for log regression:", auc_pr(train$purchase, log_pred$`1`),"\n","auc for naive bayes:", auc_pr(train$purchase, nb.prob$`1`))
#cat("auc for knn:", auc_pr(train$purchase, knn.prob$`1`))
```

Best Model: Log Regression
Understanding the model
```{r}
log_fit
plot(var.imp, top=19)
```

Apply the best model to the test dataset 
```{r}
#precision is a function to find the precision of a logistics model with a given threshold
#precision is the fraction of the predicted positives are true positives
precision <- function(threshold,model, data, field){
  response <- predict(model,newdata = data,type="prob")
  response<-response$`1`
  prediction <- ifelse(response>threshold,1,0)
  num.predicted.postive <- sum(prediction == 1)
  num.correct.positive <- sum(prediction == 1 & prediction == data[,field])
  return (num.correct.positive/num.predicted.postive)
}

#recall is a function to find the recall of a logisitc model with a given threshold
#recall is the fraction of the true positives the classifier finds
recall <- function(threshold,model, data, field){
  response <- predict(model,newdata = data,type="prob")
  response<-response$`1`
  prediction <- ifelse(response>threshold,1,0)
  num.predicted.postive <- sum(prediction == 1 & prediction == data[,field])
  num.true.positive <- sum(data[,field] == 1)
  return (num.predicted.postive/num.true.positive)
}

#This function is to plot precision and recall against thresholds
plotPrecisionVSRecall <- function(model, data, field){
  require(ggplot2)
  thresholds <- seq(0,1,by=0.01)
  precisions <- sapply(thresholds,precision,model,data,field)
  recalls <- sapply(thresholds,recall,model,data,field)
  df <- data.frame(Threshold=thresholds,Precision=precisions,Recall=recalls)
  library(reshape2)
  df1<-melt(df,id.vars="Threshold",variable.name="Type")
  ggplot(data = df1,aes(x=Threshold, y=value, color=Type,lty = Type)) + geom_line()
}

```

```{r}
plotPrecisionVSRecall(log_fit,test,"purchase")
```

From the intersection point, a threshold of 0.25 might get the best precision and recall

```{r}
best.pred <- predict(log_fit,test,type = "prob")
ctab.test <- table(pred=best.pred$`1`> 0.25, actual=test$purchase)
ctab.test
```

```{r}
precision <- ctab.test[2,2]/sum(ctab.test[2,])
recall <- ctab.test[2,2]/sum(ctab.test[,2])
cat("precison:",precision,"\n","recall:",recall)
```

